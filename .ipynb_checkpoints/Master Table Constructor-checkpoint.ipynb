{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Table Constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this section creates the data for the San Francisco Planning Department's Neighborhood Profiles Interactive Tool. The Neighborhood Profiles Interactive Tool (SFNP) provides data and information about communities in the city, including socio-economic profiles derived from American Community Survey to list of community organizations and planning projects in different areas in San Francisco. This notebook creates a master data table that contains every data point required by SFNP. The code is based off methods created by Michael Webster, Jason Sherba, and others. Run the notebook to:\n",
    "\n",
    "- Download ACS data using the Census API\n",
    "- Calculate socio-economic summary data by geographies (Analysis Neighborood, Census Tract), by race/ethnicity groups from the lastest ACS 5 years and some past surveys. \n",
    "- Integrate non-ACS data regarding community organizations and planning efforts to the ACS summary data. \n",
    "- Export the final data as a csv file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.__version__\n",
    "np.__path__\n",
    "import sys\n",
    "sys.version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import sodapy\n",
    "from sodapy import Socrata\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "from arcgis.gis import GIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. ACS 5-year data\n",
    "\n",
    "The code in this section creates the socio-economic profile data for Analysis Neighborhoods and census tracts in SF. The data is derived from the [American Community Survey](https://www.census.gov/programs-surveys/acs) 5-year data and consists of four groups of data which includes: \n",
    "\n",
    "- Data of Total Population (current year)\n",
    "    - Language Spoken Data (Detailed)\n",
    "- Data of Race/Ethnicity Groups (current year) \n",
    "\n",
    "- Data of Total Population (10 years prior to current year)\n",
    "- Data of Race/Ethnicity Groups (10 years prior to current year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set analysis year\n",
    "\n",
    "Set the years below following the instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2020 # the current year \n",
    "year_past = 2010 # 10 years past to the current year \n",
    "year_language= 2015 # the latest year in which the detailed language spoken data was available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve data from Census API\n",
    "All socio-economic data comes from the Census ACS 5-year estimates and is available at the tract level through the census API. API documentation and data for the 2020 ACS data and previous years is available [here](https://www.census.gov/data/developers/data-sets/acs-5year.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Census Attribute IDs\n",
    "The census API returns ACS attribute vales for provided attribute IDs. A list of relevant attribute ID's needed for calculating the socio-economic profile data is compiled below from IDs stored in a series of csv files named as attribute lookup tables. Below is the pairs of data and lookup tables needed to compile the data:\n",
    "\n",
    "- Data of Total Population (current year): [attribute_lookup.csv](https://github.com/jsherba/socio-economic-profiles/blob/main/lookup_tables/attribute_lookup.csv)\n",
    "    - Language Spoken Data (Detailed): [language_attribute_lookup.csv](https://github.com/jsherba/socio-economic-profiles/blob/main/lookup_tables/language_attribute_lookup.csv) \n",
    "- Data of Race/Ethnicity Groups (current year): [race_attribute_lookup.csv](https://github.com/jsherba/socio-economic-profiles/blob/main/lookup_tables/language_attribute_lookup.csv) \n",
    "- Data of Total Population (10 years prior to current year): [attribute_lookup_past.csv](https://github.com/jsherba/socio-economic-profiles/blob/main/lookup_tables/language_attribute_lookup.csv) \n",
    "- Data of Race/Ethnicity Groups (10 years prior to current year): [race_attribute_lookup_past.csv](https://github.com/jsherba/socio-economic-profiles/blob/main/lookup_tables/language_attribute_lookup.csv) \n",
    "\n",
    "For a full list of ACS attribute IDs and their meanings visit the API docs [here](https://api.census.gov/data/2019/acs/acs5/variables.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Total Population\n",
    "# Create list of attribute IDs from attribute_lookup.csv\n",
    "\n",
    "attribute_lookup_df = pd.read_csv (r'./lookup_tables/attribute_lookup.csv', dtype=str)\n",
    "\n",
    "attribute_ids_extracted = attribute_lookup_df['attribute_id'].tolist()\n",
    "attribute_ids = []\n",
    "for attribute_id in attribute_ids_extracted:\n",
    "    attribute_ids.extend(attribute_id.split(\", \"))\n",
    "attribute_ids = list(set([x+\"E\" for x in attribute_ids]))\n",
    "print(len(attribute_ids))\n",
    "attribute_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Language Spoken (Detailed)\n",
    "# Create list of attribute IDs from language_attribute_lookup.csv\n",
    "\n",
    "language_attribute_lookup_df = pd.read_csv (r'./lookup_tables/language_attribute_lookup.csv', dtype=str)\n",
    "\n",
    "language_attribute_ids_extracted = language_attribute_lookup_df['attribute_id'].tolist()\n",
    "language_attribute_ids = []\n",
    "for language_attribute_id in language_attribute_ids_extracted:\n",
    "    language_attribute_ids.extend(language_attribute_id.split(\", \"))\n",
    "language_attribute_ids = list(set([x+\"E\" for x in language_attribute_ids]))\n",
    "print(len(language_attribute_ids))\n",
    "language_attribute_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Race/Ethnicity Groups\n",
    "# Create list of attribute IDs from language_attribute_lookup.csv\n",
    "\n",
    "race_attribute_lookup_df = pd.read_csv(r'./lookup_tables/race_attribute_lookup.csv', dtype=str)\n",
    "\n",
    "race_attribute_ids_extracted = race_attribute_lookup_df['attribute_id'].tolist()\n",
    "race_attribute_ids = []\n",
    "for race_attribute_id in race_attribute_ids_extracted:\n",
    "    race_attribute_ids.extend(race_attribute_id.split(\", \"))\n",
    "race_attribute_ids = list(set([x+\"E\" for x in race_attribute_ids]))\n",
    "print(len(race_attribute_ids))\n",
    "race_attribute_ids[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Census API URL and Make Query\n",
    "The code below builds the URL for the census API call to get relevant ACS attribute data at the tract level for San Francisco County. The Census API accepts up to 50 attributes at a time. Therefore the attribute list is first grouped into sublists of 45 attribute IDs. An API call is. Below define:\n",
    "- Tract code is '*' to collect all tracts\n",
    "- State code is '06' for CA\n",
    "- County code is '075' for San Francisco County\n",
    "- Attributes are defined by the attribute id list and includes all relevant attributes for the socio-economic data calcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up functions for API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function builds the api URL from tract_code, state_code, county_code, and attribute ids. \n",
    "def build_census_url(tract_code, state_code, county_code, attribute_ids, year):\n",
    "    attributes = ','.join(attribute_ids)\n",
    "    census_url = r'https://api.census.gov/data/{}/acs/acs5?get={}&for=tract:{}&in=state:{}&in=county:{}'\\\n",
    "                .format(year, attributes, tract_code, state_code, county_code)\n",
    "    return census_url\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function makes a single api call and collects results in a pandas dataframe\n",
    "def make_census_api_call(census_url):\n",
    "    # make API call to Census\n",
    "    resp = requests.get(census_url)\n",
    "    if resp.status_code != 200:\n",
    "        # this means something went wrong\n",
    "        resp.raise_for_status()\n",
    "       \n",
    "    # retrieve data as json and convert to Pandas Dataframe\n",
    "    data = resp.json()\n",
    "    headers = data.pop(0)\n",
    "    df = pd.DataFrame(data, columns=headers)\n",
    "\n",
    "    # convert values that are not state, county, or tract to numeric type\n",
    "    cols=[i for i in df.columns if i not in [\"state\",\"county\",\"tract\"]]\n",
    "    for col in cols:\n",
    "        #print('col is:', df[col])\n",
    "        #print(type(df[col]))\n",
    "        df[col]=pd.to_numeric(df[col])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set geo variables for api call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tract_code = \"*\"\n",
    "state_code = \"06\"\n",
    "county_code = \"075\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile data: Total Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split attributes into groups of 45, run a census query for each, merge outputs into a single df\n",
    "split_attribute_ids = [attribute_ids[i:i+45] for i in range(0, len(attribute_ids), 45)]\n",
    "df=None\n",
    "first = True\n",
    "for ids in split_attribute_ids:\n",
    "    census_url = build_census_url(tract_code, state_code, county_code, ids, year)\n",
    "    returned_df = make_census_api_call(census_url)\n",
    "    if first:\n",
    "        df = returned_df\n",
    "        first = False\n",
    "    else:\n",
    "        returned_df = returned_df.drop(columns=['state', 'county'])\n",
    "        df = pd.merge(df, returned_df, on='tract', how='left')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile data: Language Spoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language: run a census query for each, merge outputs into a single df\n",
    "split_language_attribute_ids = [language_attribute_ids[i:i+45] for i in range(0, len(language_attribute_ids), 45)]\n",
    "\n",
    "first = False\n",
    "for ids in split_language_attribute_ids:\n",
    "    census_url = build_census_url(tract_code, state_code, county_code, ids, year_language)\n",
    "    #print(census_url)\n",
    "    returned_df = make_census_api_call(census_url)\n",
    "    if first:\n",
    "        df = returned_df\n",
    "        first = False\n",
    "    else:\n",
    "        returned_df = returned_df.drop(columns=['state', 'county'])\n",
    "        df = pd.merge(df, returned_df, on='tract', how='left')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile data: Race/Ethnicity Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# race/ethnicity: run a census query for each, merge outputs into a single df\n",
    "# if a call returns 'Bad Request for url', \n",
    "# click the url in the error message, look up the 'unknown variable' in the attribute lookups \n",
    "split_race_attribute_ids = [race_attribute_ids[i:i+45] for i in range(0, len(race_attribute_ids), 45)]\n",
    "\n",
    "first = False\n",
    "for ids in split_race_attribute_ids:\n",
    "    census_url = build_census_url(tract_code, state_code, county_code, ids, year)\n",
    "    #print(census_url)\n",
    "    returned_df = make_census_api_call(census_url)\n",
    "    if first:\n",
    "        df = returned_df\n",
    "        first = False\n",
    "    else:\n",
    "        returned_df = returned_df.drop(columns=['state', 'county'])\n",
    "        df = pd.merge(df, returned_df, on='tract', how='left')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Lookup Dictionaries and Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tract/Neighborhood Lookup\n",
    "A lookup dictionaries are created below that relates neghborhoods to tracts. The dictionary is used to subset the census dataframe for each neighborhood so that calcs can be run on each set of tracts. The lookup dictionary is created from [geo_lookup.csv]() in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import geo_lookup csv\n",
    "geo_lookup_df = pd.read_csv (r'./lookup_tables/geo_lookup_2020.csv', dtype=str)\n",
    "\n",
    "tract_tr_lookup = defaultdict(list)\n",
    "tract_nb_lookup = defaultdict(list)\n",
    "tract_sd_lookup = defaultdict(list)\n",
    "all_tracts = list(set(df['tract'].tolist()))\n",
    "# create tract lookup dictionary for tracts \n",
    "for i in all_tracts:\n",
    "    tract_tr_lookup[i].append(i)\n",
    "# create tract lookup dictionary for neighborhoods\n",
    "for i, j in zip(geo_lookup_df['neighborhood'], geo_lookup_df['tractid']):\n",
    "    tract_nb_lookup[i].append(j)\n",
    "tract_nb_lookup[\"sf\"]= all_tracts\n",
    "    \n",
    "first_4 = list(tract_nb_lookup.items())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Medians\n",
    "To calculate median values of aggregated geographies you cannot use the mean of component geographies. Instead a statistical approximation of the median must be calculated from range tables. \n",
    "\n",
    "Range variables in the ACS have a unique ID like any other Census variable. They represent the amount of a variable within a select range. e.g. number of households with household incomes between $45000-50000. \n",
    "\n",
    "Range variable ID's and range information is stored in the [median_ranges.csv]() file and [median_ranges_race.csv]() in the repository. These range variables and ranges are needed for calculating the median at the neighborhood level. \n",
    "\n",
    "The below function calculates a median based on range data. This method follows the offical ACS documentation for [calculating a median](https://www.dof.ca.gov/Forecasting/Demographics/Census_Data_Center_Network/documents/How_to_Recalculate_a_Median.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Medians for Total Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#all population: import median tables from median_ranges csv and add empty columns for rows 'households and 'cumulative_totals'\n",
    "range_all_df = pd.read_csv (r'./lookup_tables/median_ranges.csv')\n",
    "range_all_df['households']=0\n",
    "range_all_df['cumulative_total']=0\n",
    "range_all_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Medians for Race/Ethnicity groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#race/ethnicity: import median tables from median_ranges_race csv and add empty columns for rows 'households and 'cumulative_totals'\n",
    "range_race_df = pd.read_csv (r'./lookup_tables/median_ranges_race.csv')\n",
    "range_race_df['households']=0\n",
    "range_race_df['cumulative_total']=0\n",
    "range_race_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Median Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define median helper function\n",
    "def calc_median(tract_df, range_df, median_to_calc):\n",
    "    \n",
    "    # subset range df for current median variable to calc\n",
    "    range_df = range_df[range_df['name']==median_to_calc]\n",
    "    print(range_df)\n",
    "    \n",
    "    # sort dataframe low to high by range start column\n",
    "    range_df = range_df.sort_values(by=['range_start'])\n",
    "    \n",
    "    # calculate households as sum of tract level households for each row based on range id\n",
    "    range_df['households'] = range_df.apply(lambda row : tract_df[row['id']].sum(), axis = 1)\n",
    "    \n",
    "    # calculate the cumulative total of households\n",
    "    range_df['cumulative_total'] = range_df['households'].cumsum()\n",
    "    \n",
    "    # calculate total households and return 0 if total households is 0\n",
    "    total_households = range_df['households'].sum()\n",
    "    \n",
    "    # if total households is 0 set median to 0\n",
    "    if total_households == 0:\n",
    "        return 0\n",
    "    \n",
    "    # calculate midpoint\n",
    "    midpoint = total_households/2\n",
    "\n",
    "    # if midpoint is below first range return median as end of first range value\n",
    "    if midpoint < range_df['cumulative_total'].min():\n",
    "        new_median = range_df['range_end'].min()\n",
    "        return new_median\n",
    "    \n",
    "    # if midpoint is above last range set median to end of last range value\n",
    "    if midpoint > range_df['cumulative_total'].max():\n",
    "        new_median = range_df['range_end'].max()\n",
    "        return new_median\n",
    "    \n",
    "    less_midpoint_df = range_df[range_df['cumulative_total']<midpoint]\n",
    "    \n",
    "    # get the single row containing the range just below the mid range by getting the row with the max range start from the subsetted median df\n",
    "    range_below_mid_range_df = less_midpoint_df[less_midpoint_df['range_start'] == less_midpoint_df['range_start'].max()]\n",
    "    \n",
    "    # get the cumulative total value for the first row of the range below mid range dictionary\n",
    "    total_hh_previous_range = range_below_mid_range_df['cumulative_total'].iloc[0]\n",
    "    hh_to_mid_range = midpoint - total_hh_previous_range\n",
    "    \n",
    "    # extract rows above midrange by subsetting median df for rows with cumulative total grearter than midpoint.\n",
    "    greater_midpoint_df = range_df[range_df['cumulative_total']>midpoint]\n",
    "    \n",
    "    # get the single row containing the mid range by getting the row with the min range start from the subsetted median df\n",
    "    mid_range_df = greater_midpoint_df[greater_midpoint_df['range_start'] == greater_midpoint_df['range_start'].min()]\n",
    "    \n",
    "    # get the households value for the first row of the mid range dictionary\n",
    "    hh_in_mid_range = mid_range_df['households'].iloc[0]\n",
    "    \n",
    "    # calculate proportion of number of households in the mid range that would be needed to get to the mid-point\n",
    "    prop_of_hh = hh_to_mid_range/hh_in_mid_range\n",
    "    \n",
    "    # calculate width of the mid range\n",
    "    width = (mid_range_df['range_end'].iloc[0]-mid_range_df['range_start'].iloc[0])+1\n",
    "    \n",
    "    # apply proportion to width of mid range\n",
    "    prop_to_width = prop_of_hh*width\n",
    "    beginning_of_mid_range = mid_range_df['range_start'].iloc[0]\n",
    "    \n",
    "    # calculate new median\n",
    "    new_median = beginning_of_mid_range + prop_to_width\n",
    "    \n",
    "    return new_median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions for calculating socio-economic data\n",
    "\n",
    "\n",
    "The `calc_socio_economic_data` function family takes tract level data from the API call and the tract/neighborhood lookup dictionary. These functions create all of the socio-economic data calcs and returns a dictionary. The calcs in this function are derived from the [Data_Items_and_Sources.xlsx](https://github.com/jsherba/socio-economic-profiles/raw/main/Data_Items_And_Sources_2019.xlsx). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation helper functions\n",
    "- `calc_sum(df, attribute_id)`: sum values of all given attributes\n",
    "- `calc_normalized(df, attribute_id, attribute_id2)`: normalized the 1st attribute value with the 2nd attribute value \n",
    "- `calc_sum_normalized(df, attribute_list, attribute_id2)`: normalized the sum of the attribute values (attribute list) by the 2nd attribute value  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define calculation helper functions\n",
    "def calc_sum(df, attribute_id):\n",
    "    return df[attribute_id].sum()\n",
    "\n",
    "def calc_normalized(df, attribute_id, attribute_id2):\n",
    "    if df[attribute_id2].sum() == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (df[attribute_id].sum()/df[attribute_id2].sum())\n",
    "\n",
    "def calc_sum_normalized(df, attribute_list, attribute_id2):\n",
    "    if df[attribute_id2].sum()==0:\n",
    "        return 0\n",
    "    else:\n",
    "        sum_of_attributes = 0\n",
    "        for attribute_id in attribute_list:\n",
    "            sum_of_attributes+=df[attribute_id].sum()\n",
    "        return sum_of_attributes/df[attribute_id2].sum()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute value calculation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function runs all calcs for each neighborhood or census tracts\n",
    "def calc_socio_economic_data(df, lookup_df, range_df, tract_lookup):\n",
    "\n",
    "    all_calc_data = defaultdict(dict) \n",
    "    attribute_ids_extracted = lookup_df['attribute_id'].tolist()\n",
    "    attribute_names = lookup_df['attribute_name'].tolist()\n",
    "    calc_types = lookup_df['calc_type'].tolist()\n",
    "    median_calc_types = lookup_df['median_type'].tolist()\n",
    "\n",
    "\n",
    "    for nb_name, tracts in tract_lookup.items():\n",
    "        # extract attribute information for tracks associated with a neighborhood\n",
    "        tract_df = df[df['tract'].isin(tracts)]\n",
    "        print(len(tract_df))\n",
    "        # build dictionary with all stats for a neighborhood\n",
    "        all_calc_data_nb = all_calc_data[nb_name]\n",
    "\n",
    "        for i in range(0, len(lookup_df)):\n",
    "            name = attribute_names[i]\n",
    "            calc = calc_types[i]\n",
    "            print(attribute_ids_extracted[i])\n",
    "            attribute_ids = attribute_ids_extracted[i].split(\", \")\n",
    "            attribute_ids = [x+\"E\" for x in attribute_ids]\n",
    "\n",
    "            if calc == 'sum':\n",
    "                new_dict = {name:calc_sum(tract_df, attribute_ids[0])}\n",
    "                all_calc_data_nb.update(new_dict) \n",
    "            elif calc == 'sum_normalized':\n",
    "                new_dict = {name:calc_sum_normalized(tract_df, attribute_ids[:-1], attribute_ids[-1])}\n",
    "                all_calc_data_nb.update(new_dict) \n",
    "            elif calc == 'normalized':\n",
    "                new_dict = {name:calc_normalized(tract_df, attribute_ids[0], attribute_ids[1])}\n",
    "                print(attribute_ids[0], attribute_ids[1])\n",
    "                all_calc_data_nb.update(new_dict) \n",
    "            elif calc == 'median':\n",
    "                median_calc = median_calc_types[i]\n",
    "                new_dict = {name:calc_median(tract_df, range_df, median_calc)}\n",
    "                all_calc_data_nb.update(new_dict) \n",
    "            elif calc == 'none':\n",
    "                new_dict = {name:np.nan}\n",
    "                all_calc_data_nb.update(new_dict) \n",
    "\n",
    "    return all_calc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caculate Socioeconomic Profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Summary Variables (Neighborhood or Census Tract) and Output Paths\n",
    "\n",
    "Now you are ready to calculate attribute values summarized at a geographic level of your selection. Set `geo_summary_variable` below as `Neighborhood` first and run the following codes. After exporting the final data table as a csv file, come back here, set the `geo_summary_variable` as `Tract` and repeat running the code until you reach the final exporting stage again.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to download csvs\n",
    "download_path = r\"./output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set geography to summarize by. If supervisor districts set geo_summary_variable to \"Superisor District\"\n",
    "geo_summary_variable = 'Neighborhood'#'Tract'#\n",
    "\n",
    "# sets geo variables based on above choice\n",
    "if geo_summary_variable == 'Tract':\n",
    "    tract_lookup = tract_tr_lookup\n",
    "    geo_path = r'./shps/tracts_2020/tracts_sf.shp'\n",
    "    geo_merge_variable = 'tractce'\n",
    "elif geo_summary_variable == 'Neighborhood':\n",
    "    tract_lookup = tract_nb_lookup\n",
    "    geo_path = r'./shps/neighborhoods/neighborhoods5/neighborhoods5.shp'\n",
    "    geo_merge_variable = 'nhood'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Socioeconomic Profiles Calcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run functions to calculate all stats and convert calc dictionary to pandas dataframe\n",
    "all_calc_data = calc_socio_economic_data(df, attribute_lookup_df, range_all_df, tract_lookup)\n",
    "print(len(all_calc_data))\n",
    "df_all_calcs = pd.DataFrame.from_dict(all_calc_data).reset_index()\n",
    "df_all_calcs.rename(columns = {'index':'Attribute'}, inplace = True) \n",
    "print(len(df_all_calcs))\n",
    "df_all_calcs.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language Spoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run functions to calculate language spoken stats and convert calc dictionary to pandas dataframe\n",
    "lan_calc_data = calc_socio_economic_data(df, language_attribute_lookup_df, range_all_df, tract_lookup)\n",
    "df_lan_calcs = pd.DataFrame.from_dict(lan_calc_data).reset_index()\n",
    "df_lan_calcs.rename(columns = {'index':'Attribute'}, inplace = True) \n",
    "print(len(df_lan_calcs))\n",
    "df_lan_calcs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Race/Ethnicity Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run functions to calculate all race/ethnicity stats, convert dictionary to dataframe, append the datafrmae to df_all_calcs\n",
    "race_calc_data = calc_socio_economic_data(df, race_attribute_lookup_df, range_race_df, tract_lookup)\n",
    "df_race_calcs = pd.DataFrame.from_dict(race_calc_data).reset_index()\n",
    "df_race_calcs.rename(columns = {'index':'Attribute'}, inplace = True)\n",
    "df_race_calcs.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine the calc results & Arrange data by geographies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_calcs_fin = pd.concat([df_all_calcs, df_lan_calcs, df_race_calcs]).reset_index(drop = True)\n",
    "print(len(df_all_calcs_fin))\n",
    "df_all_calcs_fin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# transpose dataset so that each row represents one geographic area\n",
    "df_all_calcs_fin_tp = df_all_calcs_fin.T.reset_index()\n",
    "df_all_calcs_fin_tp.columns = df_all_calcs_fin_tp.iloc[0]\n",
    "df_all_calcs_fin_tp = df_all_calcs_fin_tp[1:].rename(columns={'Attribute': geo_summary_variable})\n",
    "df_all_calcs_fin_tp = df_all_calcs_fin_tp.sort_values(by=[geo_summary_variable]).reset_index(drop = True)\n",
    "df_all_calcs_fin_tp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Midpoint export (Keep proceeding until the final export) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export dataset to csv\n",
    "df_all_calcs_fin_tp.to_csv(os.path.join(download_path,geo_summary_variable+\"_\"+'profiles_by_geo_{}.csv'.format(year)), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat: 10 years ago"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below basically repeats the same data compiling process as above, yet the data is from `year_past` set at the beginning of this notebook. The code needs new lookup tables that include attribute IDs corresponding to the past ACS data: \n",
    "\n",
    "- [attribute_lookup_past.csv]() \n",
    "- [race_attribute_loookup_past.csv]()\n",
    "- [median_ranges_past.csv]()\n",
    "- [race_median_ranges_past.csv]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Census Attribute IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create list of attribute IDs from attribute_lookup_past.csv\n",
    "attribute_lookup_past_df = pd.read_csv (r'./lookup_tables/attribute_lookup_past.csv', dtype=str)\n",
    "attribute_ids_past_extracted = attribute_lookup_past_df['attribute_id'].tolist()\n",
    "attribute_ids_past = []\n",
    "for attribute_id in attribute_ids_past_extracted:\n",
    "    attribute_ids_past.extend(attribute_id.split(\", \"))\n",
    "attribute_ids_past = list(set([x+\"E\" for x in attribute_ids_past]))\n",
    "print(len(attribute_ids_past))\n",
    "attribute_ids_past[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create list of attribute IDs from race_attribute_lookup_past.csv\n",
    "race_attribute_lookup_past_df = pd.read_csv(r'./lookup_tables/race_attribute_lookup_past.csv', dtype=str)\n",
    "race_attribute_ids_past_extracted = race_attribute_lookup_past_df['attribute_id'].tolist()\n",
    "race_attribute_ids_past = []\n",
    "for race_attribute_id in race_attribute_ids_past_extracted:\n",
    "    race_attribute_ids_past.extend(race_attribute_id.split(\", \"))\n",
    "race_attribute_ids_past = list(set([x+\"E\" for x in race_attribute_ids_past]))\n",
    "print(len(race_attribute_ids_past))\n",
    "race_attribute_ids_past[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import median tables from median_ranges_past csv and add empty columns for rows 'households and 'cumulative_totals'\n",
    "range_past_df = pd.read_csv (r'./lookup_tables/median_ranges_past.csv')\n",
    "range_past_df['households']=0\n",
    "range_past_df['cumulative_total']=0\n",
    "range_past_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import median tables from median_ranges_past_race csv and add empty columns for rows 'households and 'cumulative_totals'\n",
    "range_past_race_df = pd.read_csv (r'./lookup_tables/median_ranges_past_race.csv')\n",
    "range_past_race_df['households']=0\n",
    "range_past_race_df['cumulative_total']=0\n",
    "range_past_race_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile data: Total Population - Past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set geo variables for api call\n",
    "tract_code = \"*\"\n",
    "state_code = \"06\"\n",
    "county_code = \"075\"\n",
    "\n",
    "# split attributes into groups of 45, run a census query for each, merge outputs into a single df\n",
    "split_attribute_ids_past = [attribute_ids_past[i:i+45] for i in range(0, len(attribute_ids_past), 45)]\n",
    "split_attribute_ids_past[:] = (value for value in split_attribute_ids_past if value != ' ')\n",
    "\n",
    "df_past = None\n",
    "first = True\n",
    "for ids in split_attribute_ids_past:\n",
    "    census_url = build_census_url(tract_code, state_code, county_code, ids, year_past)\n",
    "    returned_df = make_census_api_call(census_url)\n",
    "    if first:\n",
    "        df_past = returned_df\n",
    "        first = False\n",
    "    else:\n",
    "        returned_df = returned_df.drop(columns=['state', 'county'])\n",
    "        df_past = pd.merge(df_past, returned_df, on='tract', how='left')\n",
    "\n",
    "df_past.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile data: Race/Ethnicity Groups - Past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# race/ethnicity: run a census query for each, merge outputs into a single df\n",
    "split_race_attribute_past_ids = [race_attribute_ids_past[i:i+45] for i in range(0, len(race_attribute_ids_past), 45)]\n",
    "\n",
    "first = False\n",
    "for ids in split_race_attribute_past_ids:\n",
    "    census_url = build_census_url(tract_code, state_code, county_code, ids, year_past)\n",
    "    #print(census_url)\n",
    "    returned_df = make_census_api_call(census_url)\n",
    "    if first:\n",
    "        df_past = returned_df\n",
    "        first = False\n",
    "    else:\n",
    "        returned_df = returned_df.drop(columns=['state', 'county'])\n",
    "        df_past = pd.merge(df_past, returned_df, on='tract', how='left')\n",
    "\n",
    "df_past.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import geo_lookup csv\n",
    "geo_lookup_df = pd.read_csv (r'./lookup_tables/geo_lookup_2010.csv', dtype=str)\n",
    "geo_lookup_df['tractid'] = geo_lookup_df['GEOID'].str[4:]\n",
    "\n",
    "tract_tr_lookup = defaultdict(list)\n",
    "tract_nb_lookup = defaultdict(list)\n",
    "all_tracts = list(set(df['tract'].tolist()))\n",
    "\n",
    "# create tract lookup dictionary for tracts \n",
    "for i in all_tracts:\n",
    "    tract_tr_lookup[i].append(i) \n",
    "# create tract lookup dictionary for neighborhoods\n",
    "for i, j in zip(geo_lookup_df['neighborhood'], geo_lookup_df['tractid']):\n",
    "    tract_nb_lookup[i].append(j)\n",
    "tract_nb_lookup[\"sf\"]= all_tracts\n",
    "\n",
    "    \n",
    "first_4 = list(tract_nb_lookup.items())\n",
    "\n",
    "\n",
    "# sets geo variables based on above choice\n",
    "if geo_summary_variable == 'Tract':\n",
    "    tract_lookup = tract_tr_lookup\n",
    "    geo_path = r'./shps/tracts_2010/geo_export_f50126ea-5b6e-4471-a97d-e00c306d6496.shp'\n",
    "    geo_merge_variable = 'tractce'\n",
    "elif geo_summary_variable == 'Neighborhood':\n",
    "    tract_lookup = tract_nb_lookup\n",
    "    geo_path = r'./shps/neighborhoods/neighborhoods5/neighborhoods5.shp'\n",
    "    geo_merge_variable = 'nhood'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tract_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Socio-economic Profiles calc - Past"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Population - Past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run functions to calculate all stats and convert calc dictionary to pandas dataframe\n",
    "all_calc_past_data = calc_socio_economic_data(df_past, attribute_lookup_past_df, range_past_df, tract_lookup)\n",
    "df_all_calcs_past = pd.DataFrame.from_dict(all_calc_past_data).reset_index()\n",
    "\n",
    "df_all_calcs_past.rename(columns = {'index':'Attribute'}, inplace = True) \n",
    "df_all_calcs_past.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Race/Ethnicity Groups - Past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run functions to calculate all race/ethnicity stats, convert dictionary to dataframe, append the datafrmae to df_all_calcs\n",
    "race_calc_past_data = calc_socio_economic_data(df_past, race_attribute_lookup_past_df, range_past_race_df, tract_lookup)\n",
    "df_race_calcs_past = pd.DataFrame.from_dict(race_calc_past_data).reset_index()\n",
    "df_race_calcs_past.rename(columns = {'index':'Attribute'}, inplace = True)\n",
    "df_race_calcs_past.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine the calc results & Arrange data by geographies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_calcs_past_fin = pd.concat([df_all_calcs_past, df_race_calcs_past]).reset_index(drop = True)\n",
    "df_all_calcs_past_fin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# transpose dataset for second geo view of dataset\n",
    "df_all_calcs_past_fin_tp = df_all_calcs_past_fin.T.reset_index()\n",
    "df_all_calcs_past_fin_tp.columns = df_all_calcs_past_fin_tp.iloc[0]\n",
    "df_all_calcs_past_fin_tp = df_all_calcs_past_fin_tp[1:].rename(columns={'Attribute': geo_summary_variable})\n",
    "df_all_calcs_past_fin_tp = df_all_calcs_past_fin_tp.sort_values(by=[geo_summary_variable])\n",
    "df_all_calcs_past_fin_tp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the socio-economic profiles - past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the view to csv\n",
    "df_all_calcs_past_fin_tp.to_csv(os.path.join(download_path,geo_summary_variable+\"_\"+'profiles_by_geo_{}.csv'.format(year_past)), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the present and past profiles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get both dataset views for the current year and 10 years ago \n",
    "data_year_df = pd.read_csv (r'./output/'+geo_summary_variable+\"_\"+'profiles_by_geo_{}.csv'.format(year), dtype=str)\n",
    "data_year2_df = pd.read_csv (r'./output/'+geo_summary_variable+\"_\"+'profiles_by_geo_{}.csv'.format(year_past), dtype=str)\n",
    "\n",
    "data_joined_years_df= data_year_df.merge(data_year2_df, on=geo_summary_variable, suffixes=(\"\", \"_10\"), how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_joined_years_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the final socio-economic profiles data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the final data view \n",
    "data_joined_years_df.to_csv(os.path.join(download_path,geo_summary_variable+\"_\"+'profiles_by_geo_{}_{}.csv'.format(year, year_past)), index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat for Tract-level Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_summary_variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the geo_summary_variable is 'Neighborhood',\n",
    "\n",
    "Go back to the top, rerun the code cell by cell, once you reach 'Calculate Socioeconomic Profiles' section, change the summary variable to 'Tract' and keep running the following code until you reach this point again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Non-Census data\n",
    "\n",
    "In SFNP, there are four groups of data that are derived from sources other than ACS: \n",
    "- Affordable Housing\n",
    "- Eviction \n",
    "- Equity Geographies/Project Boundaries \n",
    "- Built Environments \n",
    "\n",
    "Unlike the ACS data, these data are either 1) manually compiled by staff in SF Planning and located under the resources folder in this repository; or 2) derived from [DataSF](https://datasf.org/opendata/). \n",
    "\n",
    "The code below does:\n",
    "- load the csv files\n",
    "- aggregate the data by geographic areas\n",
    "- add the result as new attributes to the socio-economic profile data created and saved by the code in the ACS 5 years section above.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the socioeconomic profiles data \n",
    "neigh_data = pd.read_csv(os.path.join(download_path,'Neighborhood_profiles_by_geo_{}_{}.csv'.format(year, year_past)))\n",
    "tract_data = pd.read_csv(os.path.join(download_path,'Tract_profiles_by_geo_{}_{}.csv'.format(year, year_past))).convert_dtypes()\n",
    "tract_data['Tract']=tract_data['Tract'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the neighborhood/census tract shapefiles as geopandas dataframe for spatial join \n",
    "tract_path = r'./shps/tracts_2020/tracts_sf.shp'\n",
    "neigh_path = r'./shps/neighborhoods/neighborhoods5/neighborhoods5.shp'\n",
    "\n",
    "tract_df = gpd.read_file(tract_path).convert_dtypes()\n",
    "neigh_df = gpd.read_file(neigh_path)\n",
    "\n",
    "tract_df['tractce'] = tract_df['tractce'].astype(int)\n",
    "\n",
    "neighborhood_list = neigh_df['nhood'].tolist()\n",
    "tract_list = tract_df['tractce'].tolist()\n",
    "\n",
    "geo_lookup_df = pd.read_csv(r'./lookup_tables/geo_lookup_{}.csv'.format(year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affordable Housing (Affordable units + SROs + Rent-controlled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data was compiled by James Papas and Michael Webster in the SF Planning Department. While the dataset provides data for 2020, there is no maintenance plan for this data yet. Three csv files were derived from the original dataset: \n",
    "    - [affordable_housing_2021_for_NP.csv]()\n",
    "    - [SRO_Points_for_NP.csv]()\n",
    "    - [rent_controlled_2019_for_NP.csv]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affordable Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the affordable housing data \n",
    "aff_df = pd.read_csv (r'./resources/affordable_housing_2021_for_NP.csv')\n",
    "aff_df['tot_units'] = aff_df.tot_units + (aff_df.tot_units == 0) * (aff_df.aff_unit)\n",
    " \n",
    "aff_df['aff_unit_ratio'] = aff_df['aff_unit']/aff_df['tot_units']\n",
    "aff_df[['aff_unit', 'tot_units', 'aff_unit_ratio']] = aff_df[['aff_unit', 'tot_units', 'aff_unit_ratio']].astype(float)\n",
    "print(aff_df['neighborhood'])\n",
    "aff_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate the number of affordable units into geographic areas (neighborhood, census tract)\n",
    "aff_df_by_neighborhood = aff_df.groupby(\"neighborhood\").agg({'aff_unit':['count','sum'],'tot_units':'sum','aff_unit_ratio':'mean'})\n",
    "aff_df_by_neighborhood.columns = ['aff_count', 'aff_unit_sum', 'aff_tot_units_sum', 'aff_mean_aff_ratio']\n",
    "aff_df_by_neighborhood = aff_df_by_neighborhood.reset_index()\n",
    "\n",
    "aff_df_by_tract = aff_df.groupby(\"tractce\").agg({'aff_unit':['count','sum'],'tot_units':'sum','aff_unit_ratio':'mean'})\n",
    "aff_df_by_tract.columns = ['aff_count', 'aff_unit_sum', 'aff_tot_units_sum', 'aff_mean_aff_ratio']\n",
    "aff_df_by_tract = aff_df_by_tract.reset_index()\n",
    "\n",
    "aff_df_by_tract.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aff_df_by_tract.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the data to the socioeconomic profiles data \n",
    "neigh_data = neigh_data.merge(aff_df_by_neighborhood, how= 'left', left_on = 'Neighborhood', right_on = 'neighborhood')\n",
    "neigh_data= neigh_data.drop(['neighborhood'], axis=1)\n",
    "neigh_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the data to the socioeconomic profiles data \n",
    "tract_data = tract_data.merge(aff_df_by_tract, how='left', left_on = 'Tract', right_on = 'tractce')\n",
    "tract_data = tract_data.drop(['tractce'], axis=1)\n",
    "tract_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SROs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the SRO data \n",
    "sro_df = pd.read_csv (r'./resources/SRO_Points_for_NP.csv')\n",
    "print(sro_df.columns)\n",
    "sro_df['residential_unit_ratio'] = sro_df['CERT_RESID']/(sro_df['CERT_RESID']+sro_df['CERT_TOURI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate the raw data into geographic units (neighborhood, census tract)\n",
    "sro_df_by_neighborhood = sro_df.groupby(\"NHOOD\").agg({'CERT_RESID':['count','sum'],'CERT_TOURI':'sum','residential_unit_ratio':'mean'})\n",
    "sro_df_by_neighborhood.columns = ['sro_count', 'sro_residential_unit', 'sro_tourist_unit', 'sro_mean_residential_ratio']\n",
    "sro_df_by_neighborhood = sro_df_by_neighborhood.reset_index()\n",
    "\n",
    "sro_df_by_tract = sro_df.groupby(\"tractce\").agg({'CERT_RESID':['count','sum'],'CERT_TOURI':'sum','residential_unit_ratio':'mean'})\n",
    "sro_df_by_tract.columns = ['sro_count', 'sro_residential_unit', 'sro_tourist_unit', 'sro_mean_residential_ratio']\n",
    "sro_df_by_tract = sro_df_by_tract.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the data to the socioeconomic profiles data \n",
    "neigh_data = neigh_data.merge(sro_df_by_neighborhood, how= 'left', left_on = 'Neighborhood', right_on = 'NHOOD')\n",
    "neigh_data= neigh_data.drop(['NHOOD'], axis=1)\n",
    "neigh_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the data to the socioeconomic profiles data \n",
    "tract_data = tract_data.merge(sro_df_by_tract, how='left', left_on = 'Tract', right_on = 'tractce')\n",
    "tract_data = tract_data.drop(['tractce'], axis=1).fillna(0)\n",
    "tract_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rent-controlled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the rent-controlled data\n",
    "rc_df = pd.read_csv (r'./resources/rent_controlled_2019_for_NP.csv')\n",
    "print(rc_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate the raw data into geographic units (neighborhood, census tract)\n",
    "rc_df_by_neighborhood = rc_df.groupby(\"NHOOD\").agg({'RESUNITS':['count','sum']})\n",
    "rc_df_by_neighborhood.columns = ['rc_count', 'rc_residential_unit']\n",
    "rc_df_by_neighborhood = rc_df_by_neighborhood.reset_index()\n",
    "\n",
    "rc_df_by_tract = rc_df.groupby(\"tractce\").agg({'RESUNITS':['count','sum']})\n",
    "rc_df_by_tract.columns = ['rc_count', 'rc_residential_unit']\n",
    "rc_df_by_tract = rc_df_by_tract.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the data to the socioeconomic profiles data \n",
    "neigh_data = neigh_data.merge(rc_df_by_neighborhood, how= 'left', left_on = 'Neighborhood', right_on = 'NHOOD')\n",
    "neigh_data= neigh_data.drop(['NHOOD'], axis=1)\n",
    "neigh_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the data to the socioeconomic profiles data \n",
    "tract_data = tract_data.merge(rc_df_by_tract, how='left', left_on = 'Tract', right_on = 'tractce')\n",
    "tract_data = tract_data.drop(['tractce'], axis=1).fillna(0)\n",
    "tract_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eviction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eviction data is derived from [xxxx dataset]() on DataSF(SF's open data portal). The code below adds number of eviction by categories as new attributes to the master data table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the eviction data from DataSF: url = 'https://data.sfgov.org/resource/5cei-gny5.geojson'\n",
    "url = \"https://data.sfgov.org/resource/5cei-gny5.geojson?$limit=45000\"\n",
    "eviction = gpd.read_file(url)\n",
    "print(eviction.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop the existing neighborhood column \n",
    "eviction = eviction.drop(['neighborhood'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run spatial join between neighborhood boundaries and pci street segment \n",
    "\n",
    "eviction_tract= eviction.sjoin(tract_df, how=\"left\", predicate='intersects')\n",
    "eviction_tract = pd.DataFrame(eviction_tract.drop(columns='geometry'))\n",
    "\n",
    "eviction_neigh= eviction.sjoin(neigh_df, how=\"left\", predicate='intersects')\n",
    "eviction_neigh = pd.DataFrame(eviction_neigh.drop(columns='geometry'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create a dictionary for aggregation \n",
    "eviction_keys = ['good_samaritan_ends','roommate_same_unit','illegal_use','lead_remediation','failure_to_sign_renewal','condo_conversion', 'nuisance',\n",
    "       'access_denial','owner_move_in', 'demolition','substantial_rehab', 'late_payments','unapproved_subtenant','capital_improvement','breach', 'development','ellis_act_withdrawal', 'other_cause',\n",
    "       'non_payment']\n",
    "eviction_values = ['sum']*19\n",
    "res = dict(zip(eviction_keys, eviction_values))\n",
    "\n",
    "# aggregate the data by neighborhoods \n",
    "eviction_df_by_neighborhood = eviction_neigh.groupby(\"nhood\").agg(res).astype(float)\n",
    "eviction_df_by_neighborhood.columns = eviction_keys\n",
    "eviction_df_by_neighborhood = eviction_df_by_neighborhood.reset_index()\n",
    "eviction_df_by_neighborhood.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate the data by tracts \n",
    "eviction_df_by_tract = eviction_tract.groupby(\"tractce\").agg(res).astype(float)\n",
    "eviction_df_by_tract.columns = eviction_keys\n",
    "eviction_df_by_tract = eviction_df_by_tract.reset_index().convert_dtypes()\n",
    "eviction_df_by_tract['tractce'] = eviction_df_by_tract['tractce'].astype(int)\n",
    "eviction_df_by_tract.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# add a row for SF \n",
    "eviction_df_sf = eviction_neigh.agg(res).astype(float)\n",
    "#pd.Series(['sf'], index = ['neighborhood'])\n",
    "eviction_df_sf = pd.Series(['sf'], index = ['nhood']).append(eviction_df_sf)\n",
    "eviction_df_by_neighborhood = eviction_df_by_neighborhood.append(eviction_df_sf,ignore_index=True)\n",
    "eviction_df_by_neighborhood.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the data to the socioeconomic profiles data \n",
    "neigh_data = neigh_data.merge(eviction_df_by_neighborhood, how= 'left', left_on = 'Neighborhood', right_on = 'nhood')\n",
    "neigh_data= neigh_data.drop(['nhood'], axis=1)\n",
    "neigh_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the data to the socioeconomic profiles data \n",
    "tract_data = tract_data.merge(eviction_df_by_tract, how='left', left_on = 'Tract', right_on = 'tractce')\n",
    "tract_data = tract_data.drop(['tractce'], axis=1).fillna(0)\n",
    "tract_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equity Geographies/Project Boundaries\n",
    "\n",
    "`np_boundaries.csv` file in thie repository contains data that shows whether various city-led projects and geographies for equitable development apply to the SF neighborhoods. The data is complied by city staff using GIS software in a way that each column in the file represent one equity geographies or project boundaries. The code below load and join the data to the master table. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the np_boundaries table \n",
    "np_boundaries = pd.read_csv (r'./resources/np_boundaries.csv')\n",
    "np_boundaries_tract = np_boundaries.merge(geo_lookup_df, how='right', left_on='nhood', right_on = 'neighborhood')\n",
    "np_boundaries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# join np_boundaries to the master table \n",
    "\n",
    "neigh_data = neigh_data.merge(np_boundaries, how= 'left', left_on = 'Neighborhood', right_on = 'nhood')\n",
    "neigh_data= neigh_data.drop(['nhood'], axis=1)\n",
    "\n",
    "neigh_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join np_boundaries to the master table \n",
    "\n",
    "tract_data = tract_data.merge(np_boundaries_tract, how='left', left_on = 'Tract', right_on = 'tractid')\n",
    "tract_data = tract_data.drop(['tractid'], axis=1)\n",
    "\n",
    "tract_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environmental Justice Area "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use your own ArcGIS Online credential for SF ArcGIS Online\n",
    "gis = GIS(\"https://sfgov.maps.arcgis.com/\", \"seolha.lee_cpc\", \"25Minhaa!?!\")\n",
    "print(f\"Connected to {gis.properties.portalHostname} as {gis.users.me.username}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the EJ area layer from SF ArcGIS Online \n",
    "# service directory: https://services.arcgis.com/Zs2aNLFN00jrS4gG/arcgis/rest/services/EJ_Communities/FeatureServer\n",
    "ej_id = '496f54079f934da28c9fea605056d971'\n",
    "ej = gis.content.get(ej_id)\n",
    "ej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the first layer of vz_2017 as 'layer'\n",
    "ej_layer = ej.layers[0]\n",
    "for f in ej_layer.properties.fields:\n",
    "    print(f['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the features in the layer as a shapefile)\n",
    "ej_layer.query(where = 'gridcode <999').sdf.spatial.to_featureclass('./resources/ej_communities.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the shapefile as a geopandas dataframe\n",
    "ej_layer_shp = gpd.read_file('./resources/ej_communities.shp')\n",
    "ej_layer_shp = ej_layer_shp.to_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate the total length of Vision Zero by neighborhood\n",
    "neigh_ej = neigh_df.sjoin(ej_layer_shp, how=\"right\", predicate='intersects')\n",
    "neigh_ej = pd.DataFrame(neigh_ej.drop(columns='geometry'))\n",
    "neigh_ej['gridcode_wt'] = neigh_ej['gridcode']*neigh_ej['shape_area']\n",
    "\n",
    "\n",
    "ej_mean_list = list()\n",
    "ej_max_list = list()\n",
    "\n",
    "for neighborhood in neighborhood_list: \n",
    "    sub_ej = neigh_ej[(neigh_ej[\"nhood\"]==neighborhood)]\n",
    "    \n",
    "    ej_sum = sub_ej['gridcode_wt'].sum()\n",
    "    area_sum = sub_ej['shape_area'].sum()\n",
    "    ej_mean = ej_sum/area_sum\n",
    "    ej_max = sub_ej['gridcode'].max()\n",
    "\n",
    "    ej_mean_list.append(ej_mean)\n",
    "    ej_max_list.append(ej_max)\n",
    "    \n",
    "    \n",
    "neigh_ej_df = pd.DataFrame({'nhood': neighborhood_list, 'ej_mean':ej_mean_list, 'ej_max':ej_max_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the total length of Vision Zero by neighborhood\n",
    "tract_ej = tract_df.sjoin(ej_layer_shp, how=\"right\", predicate='intersects')\n",
    "tract_ej = pd.DataFrame(tract_ej.drop(columns='geometry'))\n",
    "tract_ej['gridcode_wt'] = tract_ej['gridcode']*tract_ej['shape_area']\n",
    "\n",
    "\n",
    "ej_mean_list = list()\n",
    "ej_max_list = list()\n",
    "\n",
    "for tract in tract_list:  \n",
    "    sub_ej = tract_ej[(tract_ej[\"tractce\"]==tract)]\n",
    "    \n",
    "    ej_sum = sub_ej['gridcode_wt'].sum()\n",
    "    area_sum = sub_ej['shape_area'].sum()\n",
    "    ej_mean = ej_sum/area_sum\n",
    "    ej_max = sub_ej['gridcode'].max()\n",
    "\n",
    "    ej_mean_list.append(ej_mean)\n",
    "    ej_max_list.append(ej_max)\n",
    "    \n",
    "    \n",
    "tract_ej_df = pd.DataFrame({'tractce': tract_list, 'ej_mean':ej_mean_list, 'ej_max':ej_max_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the 'vz_length' as a column to the master table\n",
    "neigh_data = neigh_data.merge(neigh_ej_df, how= 'left', left_on = 'Neighborhood', right_on = 'nhood')\n",
    "neigh_data= neigh_data.drop(['nhood'], axis=1)\n",
    "\n",
    "neigh_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the 'vz_length' as a column to the master table\n",
    "tract_data = tract_data.merge(tract_vz_df, how='left', left_on = 'Tract', right_on = 'tractce')\n",
    "tract_data = tract_data.drop(['tractce'], axis=1)\n",
    "\n",
    "tract_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built Environment\n",
    "\n",
    "SFNP also provides indicators that summarises the qulity of built environment and the number of community amenities in the neighborhooods. The code below calcualtes three indicators, using datasets found on [DataSF] and SF Planning ArcGIS Online and the neighborhood boundaries shapefile in this repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pavement Condition Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the PCI data from DataSF\n",
    "\n",
    "url = \"https://data.sfgov.org/resource/5aye-4rtt.geojson?$limit=45000\"\n",
    "pci = gpd.read_file(url)\n",
    "pci['length'] = pci['geometry'].length\n",
    "\n",
    "# filter items that has length > 0 (street segments)\n",
    "pci = pci[pci['length'].notna()]\n",
    "pci.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run spatial join between neighborhood boundaries and pci street segment \n",
    "\n",
    "neigh_pci = neigh_df.sjoin(pci, how=\"right\", predicate='intersects')\n",
    "neigh_pci = pd.DataFrame(neigh_pci.drop(columns='geometry'))\n",
    "\n",
    "# calculate high_pci_ratio: high pci - PCI > 85 \n",
    "\n",
    "high_pci_ratio = list()\n",
    "for neighborhood in neighborhood_list: \n",
    "    total_len = neigh_pci['length'].sum()\n",
    "    high_pci = neigh_pci[(neigh_pci[\"nhood\"]==neighborhood) & (neigh_pci[\"pci_score\"].astype(float)>85)]\n",
    "    high_len = high_pci['length'].sum()\n",
    "    high_pci_ratio.append(high_len/total_len)\n",
    "        \n",
    "        \n",
    "high_pci_df_neigh = pd.DataFrame({'nhood': neighborhood_list, 'high_pci_ratio':high_pci_ratio})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run spatial join between neighborhood boundaries and pci street segment \n",
    "\n",
    "tract_pci = tract_df.sjoin(pci, how=\"right\", predicate='intersects')\n",
    "tract_pci = pd.DataFrame(tract_pci.drop(columns='geometry'))\n",
    "\n",
    "# calculate high_pci_ratio: high pci - PCI > 85 \n",
    "\n",
    "high_pci_ratio = list()\n",
    "for tract in tract_list: \n",
    "    total_len = tract_pci['length'].sum()\n",
    "    high_pci = tract_pci[(tract_pci[\"tractce\"]==tract) & (tract_pci[\"pci_score\"].astype(float)>85)]\n",
    "    high_len = high_pci['length'].sum()\n",
    "    high_pci_ratio.append(high_len/total_len)\n",
    "        \n",
    "        \n",
    "high_pci_df_tract = pd.DataFrame({'tractce': tract_list, 'high_pci_ratio':high_pci_ratio}).convert_dtypes()\n",
    "high_pci_df_tract['tractce'] = high_pci_df_tract['tractce'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the 'high_pci_ratio' as a column to the master table\n",
    "\n",
    "neigh_data = neigh_data.merge(high_pci_df_neigh, how= 'left', left_on = 'Neighborhood', right_on = 'nhood')\n",
    "neigh_data= neigh_data.drop(['nhood'], axis=1)\n",
    "\n",
    "neigh_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the 'high_pci_ratio' as a column to the master table\n",
    "\n",
    "tract_data = tract_data.merge(high_pci_df_tract, how='left', left_on = 'Tract', right_on = 'tractce')\n",
    "tract_data = tract_data.drop(['tractce'], axis=1)\n",
    "\n",
    "tract_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hign Injury Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use your own ArcGIS Online credential for SF ArcGIS Online\n",
    "gis = GIS(\"https://sfgov.maps.arcgis.com/\", \"seolha.lee_cpc\", \"25Minhaa!?!\")\n",
    "print(f\"Connected to {gis.properties.portalHostname} as {gis.users.me.username}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the Vision Zero 2017 high injury network data from SF ArcGIS Online \n",
    "# service directory: https://services.arcgis.com/Zs2aNLFN00jrS4gG/arcgis/rest/services/vz_hin_2017_single_line/FeatureServer\n",
    "vz_2017_id = '25d06501f18e458491ca7c6d4e3813b4'\n",
    "vz_2017 = gis.content.get(vz_2017_id)\n",
    "vz_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the first layer of vz_2017 as 'layer'\n",
    "layer = vz_2017.layers[0]\n",
    "for f in layer.properties.fields:\n",
    "    print(f['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the features in the layer as a shapefile\n",
    "features = layer.query(where = 'length >0')\n",
    "features.sdf.spatial.to_featureclass('./resources/vz_2017.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the shapefile as a geopandas dataframe\n",
    "vz_2017_shp = gpd.read_file('./resources/vz_2017.shp')\n",
    "vz_2017_shp = vz_2017_shp.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# load the street centerline as a geopandas dataframe\n",
    "st_ctl = gpd.read_file('./resources/Street_Centerline.shp').drop(['nhood'], axis=1)\n",
    "st_ctl = st_ctl.to_crs(\"EPSG:4326\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate the total length of Vision Zero by neighborhood\n",
    "neigh_vz_2017 = neigh_df.sjoin(vz_2017_shp, how=\"right\", predicate='intersects')\n",
    "neigh_vz_2017 = pd.DataFrame(neigh_vz_2017.drop(columns='geometry'))\n",
    "\n",
    "neigh_st_ctl = neigh_df.sjoin(st_ctl, how='right', predicate='intersects')\n",
    "neigh_st_ctl = pd.DataFrame(neigh_st_ctl.drop(columns='geometry'))\n",
    "print(neigh_st_ctl.columns)\n",
    "\n",
    "vz_length = list()\n",
    "st_length = list()\n",
    "vz_ratio_list = list()\n",
    "\n",
    "for neighborhood in neighborhood_list: \n",
    "    sub_vz = neigh_vz_2017[(neigh_vz_2017[\"nhood\"]==neighborhood)]\n",
    "    sub_st = neigh_st_ctl[(neigh_st_ctl['nhood']==neighborhood)]\n",
    "    total_vz_len = sub_vz['length'].sum()\n",
    "    total_st_len = sub_st['st_length_'].sum()\n",
    "    vz_ratio = total_vz_len/total_st_len\n",
    "    \n",
    "    vz_length.append(total_vz_len)\n",
    "    st_length.append(total_st_len)\n",
    "    vz_ratio_list.append(vz_ratio)\n",
    "    \n",
    "    \n",
    "        \n",
    "neigh_vz_df = pd.DataFrame({'nhood': neighborhood_list, 'vz_length':vz_length, 'st_length':st_length, 'vz_ratio':vz_ratio_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the total length of Vision Zero by tract\n",
    "tract_vz_2017 = tract_df.sjoin(vz_2017_shp, how=\"right\", predicate='intersects')\n",
    "tract_vz_2017 = pd.DataFrame(tract_vz_2017.drop(columns='geometry'))\n",
    "\n",
    "tract_st_ctl = tract_df.sjoin(st_ctl, how='right', predicate='intersects')\n",
    "tract_st_ctl = pd.DataFrame(tract_st_ctl.drop(columns='geometry'))\n",
    "print(tract_st_ctl.columns)\n",
    "\n",
    "vz_length = list()\n",
    "st_length = list()\n",
    "vz_ratio_list = list()\n",
    "\n",
    "for tract in tract_list: \n",
    "    sub_vz = tract_vz_2017[(tract_vz_2017[\"tractce\"]==tract)]\n",
    "    sub_st = tract_st_ctl[(tract_st_ctl['tractce']==tract)]\n",
    "    total_vz_len = sub_vz['length'].sum()\n",
    "    total_st_len = sub_st['st_length_'].sum()\n",
    "    vz_ratio = total_vz_len/total_st_len\n",
    "    \n",
    "    vz_length.append(total_vz_len)\n",
    "    st_length.append(total_st_len)\n",
    "    vz_ratio_list.append(vz_ratio)\n",
    "    \n",
    "tract_vz_df = pd.DataFrame({'tractce': tract_list, 'vz_length':vz_length, 'st_length':st_length, 'vz_ratio':vz_ratio_list}).convert_dtypes()\n",
    "tract_vz_df['tractce'] = tract_vz_df['tractce'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the 'vz_length' as a column to the master table\n",
    "neigh_data = neigh_data.merge(neigh_vz_df, how= 'left', left_on = 'Neighborhood', right_on = 'nhood')\n",
    "neigh_data= neigh_data.drop(['nhood'], axis=1)\n",
    "\n",
    "neigh_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the 'vz_length' as a column to the master table\n",
    "tract_data = tract_data.merge(tract_vz_df, how='left', left_on = 'Tract', right_on = 'tractce')\n",
    "tract_data = tract_data.drop(['tractce'], axis=1)\n",
    "\n",
    "tract_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community Facilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rec and Park facilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the recreation and park facilities data from DataSF \n",
    "# https://data.sfgov.org/resource/gtr9-ntp6.geojson \n",
    "url = \"https://data.sfgov.org/resource/gtr9-ntp6.geojson?$limit=45000\"\n",
    "rec = gpd.read_file(url)\n",
    "rec = rec.to_crs(\"EPSG:4326\")\n",
    "rec.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# join the neighborhood boundaries to the rec data \n",
    "neigh_rec = neigh_df.sjoin(rec[['objectid','propertytype', 'geometry']], how=\"right\", predicate='intersects')\n",
    "neigh_rec = pd.DataFrame(neigh_rec.drop(columns='geometry'))\n",
    "\n",
    "propertytype_list = neigh_rec['propertytype'].unique()\n",
    "\n",
    "# count the number of rec facilities in each neighborhood\n",
    "rec_count = pd.DataFrame({'propertytype':propertytype_list})\n",
    "for neighborhood in neighborhood_list: \n",
    "    sub = neigh_rec[(neigh_rec[\"nhood\"]==neighborhood)].groupby('propertytype').agg({'nhood':'count'})\n",
    "    sub.columns = [neighborhood]\n",
    "    sub = sub.reset_index()\n",
    "    rec_count = rec_count.merge(sub, on = 'propertytype', how='left') \n",
    "\n",
    "rec_count_neigh = rec_count.T.reset_index()\n",
    "rec_count_neigh.columns = rec_count_neigh.iloc[0]\n",
    "rec_count_neigh = rec_count_neigh[1:].rename(columns={'propertytype': 'Neighborhood'}).replace(np.nan, 0)\n",
    "rec_count_neigh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# join the tract boundaries to the rec data \n",
    "tract_rec = tract_df.sjoin(rec[['objectid','propertytype', 'geometry']], how=\"right\", predicate='intersects')\n",
    "tract_rec = pd.DataFrame(tract_rec.drop(columns='geometry'))\n",
    "\n",
    "propertytype_list = tract_rec['propertytype'].unique()\n",
    "\n",
    "# count the number of rec facilities in each neighborhood\n",
    "rec_count = pd.DataFrame({'propertytype':propertytype_list})\n",
    "for tract in tract_list: \n",
    "    sub = tract_rec[(tract_rec[\"tractce\"]==tract)].groupby('propertytype').agg({'tractce':'count'})\n",
    "    sub.columns = [tract]\n",
    "    sub = sub.reset_index()\n",
    "    rec_count = rec_count.merge(sub, on = 'propertytype', how='left') \n",
    "\n",
    "rec_count_tract = rec_count.T.reset_index()\n",
    "rec_count_tract.columns = rec_count_tract.iloc[0]\n",
    "rec_count_tract = rec_count_tract[1:].rename(columns={'propertytype': 'Tract'}).replace(np.nan, 0)\n",
    "rec_count_tract['Tract'] = rec_count_tract['Tract'].astype(int)\n",
    "rec_count_tract.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the 'vz_length' as a column to the master table\n",
    "neigh_data = neigh_data.merge(rec_count_neigh, how= 'left', on = 'Neighborhood')\n",
    "neigh_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the 'vz_length' as a column to the master table\n",
    "tract_data = tract_data.merge(rec_count_tract, how='left', on = 'Tract')\n",
    "tract_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the school data from DataSF as a geopandas point data\n",
    "# https://data.sfgov.org/resource/gtr9-ntp6.geojson \n",
    "url = \"https://data.sfgov.org/resource/rxa4-qmcf.json\"\n",
    "\n",
    "school = pd.read_json(url)\n",
    "school_df = gpd.GeoDataFrame(school, geometry=gpd.points_from_xy(school.longitude, school.latitude, crs=\"EPSG:4326\"))\n",
    "school_df = school_df[school_df['common_name'].str.contains('High|Elementary|Middle')].reset_index(drop = True)\n",
    "print(school_df.dtypes)\n",
    "school_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial join the school point data to the neighborhood boundaries \n",
    "neigh_school = neigh_df.sjoin(school_df[['facility_id', 'common_name', 'geometry']], how=\"right\", predicate='intersects')\n",
    "neigh_school = pd.DataFrame(neigh_school.drop(columns='geometry'))\n",
    "neigh_school.dtypes\n",
    "\n",
    "tract_school = tract_df.sjoin(school_df[['facility_id', 'common_name', 'geometry']], how=\"right\", predicate='intersects')\n",
    "tract_school = pd.DataFrame(tract_school.drop(columns='geometry'))\n",
    "tract_school.dtypes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate the number of schools by neighborhood \n",
    "school_count_list = []\n",
    "for neighborhood in neighborhood_list: \n",
    "    sub = neigh_school[(neigh_school[\"nhood\"]==neighborhood)]\n",
    "    school_count_list.append(len(sub))\n",
    "    sub = sub.reset_index()\n",
    "    \n",
    "school_count_neigh = pd.DataFrame({'Neighborhood':neighborhood_list,\n",
    "                               'school':school_count_list})\n",
    "school_count_neigh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of schools by neighborhood \n",
    "school_count_list = []\n",
    "for tract in tract_list: \n",
    "    sub = tract_school[(tract_school[\"tractce\"]==tract)]\n",
    "    school_count_list.append(len(sub))\n",
    "    sub = sub.reset_index()\n",
    "    \n",
    "school_count_tract = pd.DataFrame({'Tract':tract_list,\n",
    "                               'school':school_count_list}).convert_dtypes()\n",
    "school_count_tract['Tract'] = school_count_tract['Tract'].astype(int)\n",
    "school_count_tract.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add the 'school' as a column to the master table\n",
    "neigh_data = neigh_data.merge(school_count_neigh, how= 'left', on = 'Neighborhood')\n",
    "neigh_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the 'school' as a column to the master table\n",
    "tract_data = tract_data.merge(school_count_tract, how='left', on = 'Tract')\n",
    "tract_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the Master Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add nhood_url \n",
    "'nhood_url' will be used to build URLs of html tables. You can't use the original neighbohroods names in URLs because some of them contain '/' in it. Here we replace '/' with '-'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add nhood_url for neighborhood names for URLs. \n",
    "neigh_data['nhood_url'] = neigh_data['Neighborhood'].str.replace('/','-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export the master tables as csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the neigh master table as csv \n",
    "neigh_data.to_csv(os.path.join(download_path,'Neighborhood_master_table_by_geo_{}_{}.csv'.format(year, year_past)), index = False)\n",
    "\n",
    "# export only for the 'sf' row \n",
    "sf_data = neigh_data[neigh_data['Neighborhood']=='sf']\n",
    "sf_data.to_csv(os.path.join(download_path,'SF_master_table_by_geo_{}_{}.csv'.format(year, year_past)), index = False)\n",
    "\n",
    "# export the tract master table as csv \n",
    "tract_data.to_csv(os.path.join(download_path,'Tract_master_table_by_geo_{}_{}.csv'.format(year, year_past)), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export the master layers as geojson files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the master tables \n",
    "neigh_data = pd.read_csv(os.path.join(download_path,'Neighborhood_master_table_by_geo_{}_{}.csv'.format(year, year_past)))\n",
    "tract_data = pd.read_csv(os.path.join(download_path,'Tract_master_table_by_geo_{}_{}.csv'.format(year, year_past)))\n",
    "\n",
    "# export the master tables as master layer geojson files \n",
    "# join dataframe to neighborhoods geodataframe by neighborhood name\n",
    "neigh_data[neigh_data.select_dtypes(np.float64).columns] = neigh_data.select_dtypes(np.float64).astype(np.float32)\n",
    "tract_data[tract_data.select_dtypes(np.float64).columns] = tract_data.select_dtypes(np.float64).astype(np.float32)\n",
    "\n",
    "\n",
    "neigh_master_geo = neigh_df.merge(neigh_data,left_on='nhood', right_on='Neighborhood')\n",
    "tract_master_geo = tract_df.merge(tract_data,left_on='tractce', right_on='Tract')\n",
    "\n",
    "# export the geodataframes as geojson files \n",
    "neigh_master_geo.to_file(os.path.join(download_path,'Neighborhood_master_layer_{}.geojson'.format(year)), driver='GeoJSON')\n",
    "tract_master_geo.to_file(os.path.join(download_path,'Tract_master_layer_{}.geojson'.format(year)), driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish Data to ArcGIS Online "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcgis \n",
    "from arcgis.features import FeatureLayer\n",
    "from arcgis.features import FeatureLayerCollection\n",
    "from arcgis.gis import GIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to SF ArcGIS Online using a valid credential\n",
    "gis = GIS(\"https://sfgov.maps.arcgis.com/\", \"seolha.lee_cpc\", \"25Minhaa!?!\")\n",
    "print(f\"Connected to {gis.properties.portalHostname} as {gis.users.me.username}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load the master layer geojson files \n",
    "neigh_master_geo_path = os.path.join(download_path,'Neighborhood_master_layer_{}.geojson'.format(year))\n",
    "tract_master_geo_path = os.path.join(download_path,'Tract_master_layer_{}.geojson'.format(year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overwrite the existing layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_result = gis.content.search('title:SFNP_master_neighborhood_2020', item_type='Feature Layer')\n",
    "test_item = search_result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature_collection = FeatureLayerCollection.fromitem(test_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_feature_collection.manager.overwrite(neigh_master_geo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish a new layer (only when there is no existing layer with the same name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#set the feature layer properties \n",
    " neigh_prop = {\"snippet\":\"\"\"This feature layer is for San Francisco Planning Neighborhood Profiles (SFNP).\"\"\",\n",
    "                    \"title\":\"SFNP_master_neighborhood_{}\".format(year),\n",
    "                    \"tags\":\"neighborhood profiles\",\n",
    "                    \"type\":\"GeoJson\"}\n",
    " tract_prop = {\"snippet\":\"\"\"This feature layer is for San Francisco Planning Neighborhood Profiles (SFNP).\"\"\",\n",
    "                    \"title\":\"SFNP_master_tract_{}\".format(year),\n",
    "                    \"tags\":\"neighborhood profiles\",\n",
    "                    \"type\":\"GeoJson\"}\n",
    "\n",
    "neigh_prop = {\"snippet\":\"\"\"This feature layer is for San Francisco Planning Neighborhood Profiles (SFNP).\"\"\",\n",
    "                   \"title\":\"test_{}\".format(year),\n",
    "                   \"tags\":\"neighborhood profiles\",\n",
    "                   \"type\":\"GeoJson\"}\n",
    "\n",
    "#publish the layers as feature layers to ArcGIS Online\n",
    "neigh_master_shp = gis.content.add(item_properties = neigh_prop, data = neigh_master_geo_path)\n",
    "tract_master_shp = gis.content.add(item_properties = tract_prop, data = tract_master_geo_path)\n",
    "\n",
    "neigh_master_shp.publish()\n",
    "tract_master_shp.publish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "789px",
    "left": "46px",
    "top": "110px",
    "width": "299px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
